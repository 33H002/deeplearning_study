{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1. [DACON] 코드 유사성 판단 시즌2 AI 경진대회\n","- C++ 코드간의 유사성을 판단할 수 있는 AI 알고리즘 개발\n","- https://dacon.io/competitions/official/236228/overview/description\n","\n","---\n","1-1. Sentence-Transformer | 101_sentence_transformer.ipynb\n","- sentence transformer(all-mpnet)를 Contrastive Learning 방식으로 파인 튜닝하여 두개의 C++ 코드 쌍이 동일한 문제를 해결하는 코드인지 유사성을 판단하는 방법"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 0. Preparation "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1711874601169,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"1bZDmvs9_vS1","outputId":"1e54f952-46fc-4b99-eb03-13f54c318386"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sun Mar 31 08:43:10 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0              44W / 400W |      2MiB / 40960MiB |      0%      Default |\n","|                                         |                      |             Disabled |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76566,"status":"ok","timestamp":1711886603088,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"tWXI1-9oKLoI","outputId":"9ccdf37b-bde0-431c-fe7f-c236f96223f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting accelerate\n","  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch_metric_learning\n","  Downloading pytorch_metric_learning-2.4.1-py3-none-any.whl (118 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pytorch_metric_learning) (1.2.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch_metric_learning) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch_metric_learning) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch_metric_learning) (3.4.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: xxhash, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, pytorch_metric_learning, accelerate\n","Successfully installed accelerate-0.28.0 datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pytorch_metric_learning-2.4.1 xxhash-3.4.1\n"]}],"source":["!pip install accelerate datasets pytorch_metric_learning"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["데이터 다운로드\n","- open.zip 파일 구성은 다음과 같음\n","\n","\ttrain_code [Folder] : 학습용으로 주어지는 500개의 문제에 대한 코드\n","\n","\t\t├ problem001 : 문제 번호\n","\n","\t\t│\t├ problem001_1.cpp : 문제(001)를 해결하려는 솔루션 코드 1\n","\n","\t\t│\t├ problem001_2.cpp : 문제(001)를 해결하려는 솔루션 코드 2\n","\n","\t\t│\t└ problem001_...\n","\n","\t\t├ problem002 : 문제 번호\n","\n","\t\t│\t├ problem002_1.cpp : 문제(002)를 해결하려는 솔루션 코드 1\n","\n","\t\t│\t├ problem002_2.cpp : 문제(002)를 해결하려는 솔루션 코드 2\n","\n","\t\t│\t└ problem002_...\n","\n","\t\t└ ...\n","\n","\ttest.csv [File] : 학습 데이터에 없는 다른 문제에 대한 코드 중에서 595000개의 Pair 쌍으로 이루어진 테스트용 데이터셋\n","\n","\t\t│\t├ pair_id : 각 pair 쌍에 부여되는 id 번호\n","\n","\t\t│\t├ code1 : 유사성을 비교할 C++ 코드 1\n","\n","\t\t│\t└ code2 : 유사성을 비교할 C++ 코드 2\n","\n","\t\t│"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10968,"status":"ok","timestamp":1711886614052,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"Ywy5q9MuKQmr","outputId":"8b7edbc3-4dc0-4a3d-e3b7-13b2672b447a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=13WixS0gfcsb7NkKGje6QZA1phPlVhKQm\n","From (redirected): https://drive.google.com/uc?id=13WixS0gfcsb7NkKGje6QZA1phPlVhKQm&confirm=t&uuid=d28b9d4a-5c94-45d3-97b5-6b29b0fd4fcc\n","To: /content/open.zip\n","100% 485M/485M [00:08<00:00, 56.5MB/s]\n"]}],"source":["!gdown 13WixS0gfcsb7NkKGje6QZA1phPlVhKQm"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2115,"status":"ok","timestamp":1711889526173,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"A7anyIBwKQi-"},"outputs":[],"source":["import torch\n","import random\n","import numpy as np\n","import torch.backends.cudnn as cudnn\n","\n","def seed_everything(seed):\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","  np.random.seed(seed)\n","  cudnn.benchmark=False\n","  cudnn.deterministic=True\n","  random.seed(seed)\n","\n","SEED = 555\n","seed_everything(SEED)\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1. Load Model\n","- all_mpnet-base-v2 (https://huggingface.co/sentence-transformers/all-mpnet-base-v2)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4770,"status":"ok","timestamp":1711889533713,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"Hkw_8_Monm7h","outputId":"a1ce5ce5-0c38-4524-b3ba-3c8c6d57cd8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence embeddings:\n","tensor([[ 0.0225, -0.0783, -0.0230,  ..., -0.0083,  0.0265, -0.0020],\n","        [ 0.0417,  0.0011, -0.0155,  ..., -0.0218, -0.0636, -0.0088]])\n"]}],"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","import torch.nn.functional as F\n","\n","#Mean Pooling - Take attention mask into account for correct averaging\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","\n","# Sentences we want sentence embeddings for\n","sentences = ['This is an example sentence', 'Each sentence is converted']\n","\n","# Load model from HuggingFace Hub\n","tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n","model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n","\n","# Tokenize sentences\n","encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n","\n","# Compute token embeddings\n","with torch.no_grad():\n","    model_output = model(**encoded_input)\n","\n","# Perform pooling\n","sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","\n","# Normalize embeddings\n","sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n","\n","print(\"Sentence embeddings:\")\n","print(sentence_embeddings)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 2. Train Data Generator"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711889537526,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"s4zpcFKeKQa3"},"outputs":[],"source":["import random\n","from zipfile import ZipFile\n","from collections import defaultdict\n","\n","class TrainDataset(torch.utils.data.Dataset):\n","  def __init__(self, zip_file_path=\"/content/open.zip\", sampling=False):\n","    super().__init__()\n","    self.zip_file_path = zip_file_path\n","\n","    with ZipFile(zip_file_path, 'r') as zipfile:\n","      train_code_folder = 'train_code/'\n","      all_files = zipfile.namelist()\n","      # zip 파일 내 모든 cpp 파일 리스트\n","      self.cpp_files = [file for file in all_files if file.startswith(train_code_folder) and file.endswith('.cpp')]\n","      # 문제 번호별로 cpp 파일 분류\n","      self.problems = defaultdict(list)\n","      for _cpp_file in self.cpp_files:\n","        problem = _cpp_file.split('/')[1]\n","        # 검증셋으로 쓸 솔루션 코드 샘플링\n","        if sampling and len(self.problems[problem])>=2:\n","          pass\n","        else:\n","          # cpp 파일을 string으로\n","          with zipfile.open(_cpp_file, 'r') as zf:\n","            cpp_code = zf.read().decode('utf-8')\n","            self.problems[problem].append(cpp_code)\n","      # 검증셋 코드 필터링\n","      if not sampling:\n","        dict((k, v[2:]) for k, v in self.problems.items() if len(v)>=4)\n","      self.problem_keys = list(self.problems.keys())\n","\n","  def __len__(self):\n","    return len(self.problem_keys)\n","\n","  def __getitem__(self, idx):\n","    '''\n","    데이터셋 호출 시, \n","    동일한 문제의 솔루션 코드 2개를 랜덤으로 추출하여 {'code1': , 'code2': }와 같이 반환 (positive pair)\n","    '''\n","    key = self.problem_keys[idx]\n","    code1, code2 = random.sample(self.problems[key], 2)\n","    return {'code1': code1, 'code2': code2}"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12790,"status":"ok","timestamp":1711874632904,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"sHDJ0yvObRLu","outputId":"81ad6a3e-1c7f-46ea-ed8e-fb88f9345fac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Size of Train Dataset: 500\n","Total Size of Valid Dataset: 500\n"]}],"source":["trn_ds = TrainDataset(zip_file_path=\"/content/open.zip\")\n","print(f'Total Size of Train Dataset: {len(trn_ds)}')\n","\n","val_ds = TrainDataset(zip_file_path=\"/content/open.zip\", sampling=True)\n","print(f'Total Size of Valid Dataset: {len(val_ds)}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 3. Train"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3818,"status":"ok","timestamp":1711889546887,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"76UudgGfYcSt"},"outputs":[],"source":["import transformers\n","from pytorch_metric_learning import losses\n","\n","class Trainer(transformers.Trainer):\n","  '''\n","  Custom Transformer Trainer 정의\n","\n","  NTXentLoss 사용하여 cpp 코드의 유사성을 학습하는 모델 훈련 (ICML'2020, A Simple Framework for Contrastive Learning of Visual Representations, https://arxiv.org/abs/2002.05709)\n","  NTXentLoss는 positive sample과의 유사도를 1과 가깝게, negative sample과의 유사도를 0과 가깝게 만드는 목적 함수이다.\n","\n","  두 코드가 주어졌을 때, 동일한 문제를 위한 코드인 경우 positive pair, 서로 다른 문제를 위한 코드인 경우 negative pair 라고 하자.\n","  \n","  예를 들어, 배치 사이즈가 3일때\n","  문제 A, B, C에 대한 위에서 정의한 학습 데이터셋(TrainDataset)을 호출하면 각각 2 개, 총 6 개의 솔루션 코드(A-1, A-2, B-1, B-2, C-1, C-2)가 생성된다.\n","  이때, 코드 A-1를 anchor라고 할 때 A-2는 anchor의 positive sample이 되고 A-2를 제외한 B-1, B-2, C-1, C-2는 negative sample이 된다. \n","  즉, anchor의 positive pair는 1 개, negative pair는 4 개이며, \n","  총 6개(=3*2)의 코드가 있으므로 배치 내 positive pair는 6개(=6*1), negative pair는 24개(=6*4)가 된다.\n","  모델은 positive pair들의 유사도를 최대화하고 negative pair들의 유사도를 최소화하는 방향으로 업데이트 된다.\n","  \n","  따라서 모델 훈련이 완료되면, 동일한 문제를 위한 코드들은 유사하게, 다른 문제를 위한 코드들은 상이하게 임베딩할 수 있다.\n","  '''\n","  def __init__(self, temperature=0.07, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","    self.loss_fn = losses.NTXentLoss(temperature=temperature)\n","\n","  def compute_loss(self, model, inputs, return_outputs=False):\n","    '''\n","    손실값 계산을 위한 임베딩\n","    '''\n","    anchor = model(**inputs['code1'])\n","    anchor_embs = mean_pooling(anchor, inputs['code1']['attention_mask'])\n","    anchor_embs = F.normalize(anchor_embs, p=2, dim=1)\n","\n","    pos = model(**inputs['code2'])\n","    pos_embs = mean_pooling(pos, inputs['code2']['attention_mask'])\n","    pos_embs = F.normalize(pos_embs, p=2, dim=1)\n","\n","    # 모든 코드들을 임베딩해주고\n","    embs = torch.cat((anchor_embs, pos_embs)) \n","    # 번호를 달아 레이블을 생성 (같은 번호인 경우 positive)\n","    indices = torch.arange(0, anchor_embs.size(0))\n","    labels = torch.cat((indices, indices))\n","\n","    loss = self.loss_fn(embs, labels)\n","    return (loss, ((anchor_embs, pos_embs), labels)) if return_outputs else loss\n","\n","  def prediction_step(self, model, inputs, prediction_loss_only, **kwargs):\n","    '''\n","    prediction_step을 지정해주지 않으면 모델의 default loss를 사용하므로\n","    위에서 정의한 loss를 사용할 수 있도록 매핑해준다 \n","    '''\n","    with torch.no_grad():\n","      if prediction_loss_only:\n","        loss = self.compute_loss(model, inputs, return_outputs=False)\n","        return (loss, None, None)\n","      else:\n","        loss, (embs, labels) = self.compute_loss(model, inputs, return_outputs=True)\n","        return (loss, embs, labels)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711886631597,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"QhfS5FXylTjl"},"outputs":[],"source":["class DataCollator:\n","  '''\n","  데이터 배치를 원하는 형태로 재배열하고 토크나이징하여 반환\n","  '''\n","  def __init__(self, tokenizer):\n","    self.tokenizer = tokenizer\n","\n","  def __call__(self, examples):\n","\n","    code1 = [example['code1'] for example in examples]\n","    code2 = [example['code2'] for example in examples]\n","\n","    code1_out = self.tokenizer(code1, truncation=True, padding=True, return_tensors='pt')\n","    code2_out = self.tokenizer(code2, truncation=True, padding=True, return_tensors='pt')\n","\n","    return dict(code1=code1_out, code2=code2_out)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1035,"status":"ok","timestamp":1711874676315,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"nqPw0wI5N0AV"},"outputs":[],"source":["save_path = './outputs'\n","data_collator = DataCollator(tokenizer)\n","\n","# trainer 생성\n","trainer = Trainer(\n","    model=model,\n","    train_dataset=trn_ds,\n","    eval_dataset=val_ds,\n","    temperature=0.07,\n","    args=transformers.TrainingArguments(\n","        per_device_train_batch_size=32,\n","        per_device_eval_batch_size=32,\n","        gradient_accumulation_steps=32,\n","        warmup_steps=10,\n","        max_steps=1000,\n","        learning_rate=2e-4,\n","        fp16=True,\n","        logging_steps=1,\n","        output_dir=save_path,\n","        optim='adamw_torch',\n","        load_best_model_at_end=True,\n","        eval_steps=10,\n","        evaluation_strategy='steps',\n","        save_strategy='steps',\n","        save_total_limit=3,\n","        remove_unused_columns=False,\n","        prediction_loss_only=True,\n","    ),\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":4005067,"status":"ok","timestamp":1711881927945,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"IgeZRCKZKQS0","outputId":"8368e4b4-477d-4db8-f3de-065a73da68a1"},"outputs":[{"name":"stderr","output_type":"stream","text":["Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='449' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 449/1000 53:48 < 1:06:19, 0.14 it/s, Epoch 448/1000]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.847200</td>\n","      <td>1.730085</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.720500</td>\n","      <td>1.484783</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.601600</td>\n","      <td>1.355141</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.539100</td>\n","      <td>1.281362</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.594300</td>\n","      <td>1.197975</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.545600</td>\n","      <td>1.149572</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.570300</td>\n","      <td>1.163779</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.495900</td>\n","      <td>1.138790</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.527100</td>\n","      <td>1.106874</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.526400</td>\n","      <td>1.094657</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.450600</td>\n","      <td>1.054729</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.429800</td>\n","      <td>1.045244</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.395400</td>\n","      <td>1.003160</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.404300</td>\n","      <td>0.996794</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.503700</td>\n","      <td>1.019602</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.484800</td>\n","      <td>1.002091</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.493300</td>\n","      <td>0.978110</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.440400</td>\n","      <td>0.979320</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.465300</td>\n","      <td>0.957828</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.391600</td>\n","      <td>0.944075</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.419600</td>\n","      <td>0.933203</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.496600</td>\n","      <td>0.948461</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.418900</td>\n","      <td>0.935304</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.554700</td>\n","      <td>0.928425</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.433800</td>\n","      <td>0.908527</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.396100</td>\n","      <td>0.904312</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.436200</td>\n","      <td>0.902476</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.340200</td>\n","      <td>0.898705</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.400600</td>\n","      <td>0.890491</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.408300</td>\n","      <td>0.866071</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.422800</td>\n","      <td>0.873050</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.439900</td>\n","      <td>0.857458</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.413800</td>\n","      <td>0.849329</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.443200</td>\n","      <td>0.842477</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.422900</td>\n","      <td>0.854074</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.376300</td>\n","      <td>0.851861</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.358700</td>\n","      <td>0.842679</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.394700</td>\n","      <td>0.855529</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.351200</td>\n","      <td>0.837301</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.395100</td>\n","      <td>0.817863</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.344600</td>\n","      <td>0.819450</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.356300</td>\n","      <td>0.813137</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.349200</td>\n","      <td>0.799584</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.370900</td>\n","      <td>0.789215</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1000/1000 2:00:36, Epoch 1000/1000]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.847200</td>\n","      <td>1.730085</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.720500</td>\n","      <td>1.484783</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.601600</td>\n","      <td>1.355141</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.539100</td>\n","      <td>1.281362</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.594300</td>\n","      <td>1.197975</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.545600</td>\n","      <td>1.149572</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.570300</td>\n","      <td>1.163779</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.495900</td>\n","      <td>1.138790</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.527100</td>\n","      <td>1.106874</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.526400</td>\n","      <td>1.094657</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.450600</td>\n","      <td>1.054729</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.429800</td>\n","      <td>1.045244</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.395400</td>\n","      <td>1.003160</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.404300</td>\n","      <td>0.996794</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.503700</td>\n","      <td>1.019602</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.484800</td>\n","      <td>1.002091</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.493300</td>\n","      <td>0.978110</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.440400</td>\n","      <td>0.979320</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.465300</td>\n","      <td>0.957828</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.391600</td>\n","      <td>0.944075</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.419600</td>\n","      <td>0.933203</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.496600</td>\n","      <td>0.948461</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.418900</td>\n","      <td>0.935304</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.554700</td>\n","      <td>0.928425</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.433800</td>\n","      <td>0.908527</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.396100</td>\n","      <td>0.904312</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.436200</td>\n","      <td>0.902476</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.340200</td>\n","      <td>0.898705</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.400600</td>\n","      <td>0.890491</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.408300</td>\n","      <td>0.866071</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.422800</td>\n","      <td>0.873050</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.439900</td>\n","      <td>0.857458</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.413800</td>\n","      <td>0.849329</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.443200</td>\n","      <td>0.842477</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.422900</td>\n","      <td>0.854074</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.376300</td>\n","      <td>0.851861</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.358700</td>\n","      <td>0.842679</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.394700</td>\n","      <td>0.855529</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.351200</td>\n","      <td>0.837301</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.395100</td>\n","      <td>0.817863</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.344600</td>\n","      <td>0.819450</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.356300</td>\n","      <td>0.813137</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.349200</td>\n","      <td>0.799584</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.370900</td>\n","      <td>0.789215</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.358300</td>\n","      <td>0.784010</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.419300</td>\n","      <td>0.757326</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.424900</td>\n","      <td>0.757742</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.431500</td>\n","      <td>0.773653</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.412500</td>\n","      <td>0.746309</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.343200</td>\n","      <td>0.744558</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.381700</td>\n","      <td>0.752701</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.348600</td>\n","      <td>0.737750</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>0.330000</td>\n","      <td>0.751806</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.336600</td>\n","      <td>0.753706</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.427400</td>\n","      <td>0.755076</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.342600</td>\n","      <td>0.728450</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.373800</td>\n","      <td>0.739665</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.367600</td>\n","      <td>0.731835</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>0.291700</td>\n","      <td>0.719487</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.382700</td>\n","      <td>0.725321</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>0.334200</td>\n","      <td>0.703713</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>0.355800</td>\n","      <td>0.687286</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>0.327500</td>\n","      <td>0.708183</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>0.330200</td>\n","      <td>0.689772</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.322100</td>\n","      <td>0.689723</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.263600</td>\n","      <td>0.677748</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>0.326700</td>\n","      <td>0.688414</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>0.364400</td>\n","      <td>0.689018</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>0.339600</td>\n","      <td>0.690605</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.364900</td>\n","      <td>0.676226</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>0.348000</td>\n","      <td>0.673820</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>0.254500</td>\n","      <td>0.664419</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>0.351300</td>\n","      <td>0.659288</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>0.347700</td>\n","      <td>0.662080</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.337000</td>\n","      <td>0.654980</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.356700</td>\n","      <td>0.652079</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>0.321000</td>\n","      <td>0.653972</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>0.314000</td>\n","      <td>0.657597</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>0.324600</td>\n","      <td>0.661995</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.328500</td>\n","      <td>0.649411</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>0.268500</td>\n","      <td>0.638518</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>0.311400</td>\n","      <td>0.640915</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>0.326600</td>\n","      <td>0.632273</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.267300</td>\n","      <td>0.631968</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.261200</td>\n","      <td>0.631798</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>0.290000</td>\n","      <td>0.624350</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>0.312300</td>\n","      <td>0.621285</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>0.329000</td>\n","      <td>0.623239</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>0.328900</td>\n","      <td>0.626287</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.307600</td>\n","      <td>0.613513</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>0.279200</td>\n","      <td>0.620845</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>0.341300</td>\n","      <td>0.617501</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>0.291000</td>\n","      <td>0.620474</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>0.303900</td>\n","      <td>0.615550</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.288700</td>\n","      <td>0.609938</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>0.283500</td>\n","      <td>0.610696</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>0.300700</td>\n","      <td>0.611759</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>0.309400</td>\n","      <td>0.610411</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>0.352000</td>\n","      <td>0.610153</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.260500</td>\n","      <td>0.609647</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["model.config.use_cache = False\n","trainer.train()\n","model.save_pretrained(save_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":502,"status":"ok","timestamp":1711889556117,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"AmRPB9Pe0HGD"},"outputs":[],"source":["import gc\n","\n","gc.collect()\n","with torch.no_grad(): torch.cuda.empty_cache()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 4. Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1019,"status":"ok","timestamp":1711889557754,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"yNx1C8Hv5KiG","outputId":"e138e2ee-8c69-4209-9fa1-ed8cea79b76d"},"outputs":[{"data":{"text/plain":["MPNetModel(\n","  (embeddings): MPNetEmbeddings(\n","    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n","    (position_embeddings): Embedding(514, 768, padding_idx=1)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): MPNetEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x MPNetLayer(\n","        (attention): MPNetAttention(\n","          (attn): MPNetSelfAttention(\n","            (q): Linear(in_features=768, out_features=768, bias=True)\n","            (k): Linear(in_features=768, out_features=768, bias=True)\n","            (v): Linear(in_features=768, out_features=768, bias=True)\n","            (o): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (intermediate): MPNetIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): MPNetOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (relative_attention_bias): Embedding(32, 12)\n","  )\n","  (pooler): MPNetPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# 저장된 모델 load\n","model = AutoModel.from_pretrained(f'{save_path}/checkpoint-1000', device_map='cuda')\n","model.requires_grad=False\n","model.eval()"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":31659,"status":"ok","timestamp":1711889592568,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"hM_wnZdGKQM8"},"outputs":[],"source":["import pandas as pd\n","from datasets import Dataset\n","\n","# open.zip 파일 내의  test.csv 파일을 읽어 데이터셋 및 데이터로더를 정의\n","with ZipFile('/content/open.zip', 'r') as zipfile:\n","  with zipfile.open('test.csv', 'r') as zf:\n","      test_df = pd.read_csv(zf)\n","\n","test_ds = Dataset.from_dict({'code1':test_df.code1.tolist(),\n","                             'code2':test_df.code2.tolist(),\n","                             'pair_id':test_df.pair_id.tolist()})\n","\n","test_dl = torch.utils.data.DataLoader(test_ds,\n","                                      batch_size=512,\n","                                      shuffle=False,\n","                                      num_workers=4)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8912299,"status":"ok","timestamp":1711898504846,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"T-ZiHdGI1xDD","outputId":"86e12a4f-9082-4dd7-d3b6-d3480eb46d7d"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1163/1163 [2:28:32<00:00,  7.66s/it]\n"]}],"source":["from tqdm import tqdm\n","\n","result = defaultdict(list)\n","\n","cos = torch.nn.CosineSimilarity(dim=1)\n","\n","for i, features in enumerate(tqdm(test_dl)):\n","  with torch.no_grad(): torch.cuda.empty_cache()\n","  gc.collect()\n","\n","  code1, code2, pair_ids = features['code1'], features['code2'], features['pair_id']\n","\n","  # cpp 코드를 토크나이징해서 input data 생성\n","  code1_tok = tokenizer(code1, truncation=True, padding=True, return_tensors='pt')\n","  code2_tok = tokenizer(code2, truncation=True, padding=True, return_tensors='pt')\n","\n","  with torch.no_grad():\n","    # 각각 임베딩\n","    code1_embs = model(**code1_tok.to('cuda'))\n","    code1_embs = mean_pooling(code1_embs, code1_tok['attention_mask'])\n","    code1_embs = F.normalize(code1_embs, p=2, dim=1)\n","\n","    code2_embs = model(**code2_tok.to('cuda'))\n","    code2_embs = mean_pooling(code2_embs, code2_tok['attention_mask'])\n","    code2_embs = F.normalize(code2_embs, p=2, dim=1)\n","\n","  # 각 페어별 코사인 유사도를 구함\n","  sims = cos(code1_embs, code2_embs).detach().cpu().numpy()\n","\n","  result['pair_id'].append(pair_ids)\n","  result['similar'].append(sims)\n","\n","result['pair_id'] = np.concatenate(result['pair_id'], 0)\n","result['similar'] = np.concatenate(result['similar'], 0)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":749,"status":"ok","timestamp":1711898506908,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"aGG8TnYf38jD"},"outputs":[],"source":["# 추론 결과를 데이터 프레임으로 변환하고 임계값(0.5)을 기준으로 0과 1로 바꾸어 파일 제출\n","result = pd.DataFrame(result)\n","result.similar = result.similar.apply(lambda x: 1 if x>=0.5 else 0)\n","result.to_csv(f'{save_path}/submission-ck1000-bin.csv', index=False)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"elapsed":587,"status":"ok","timestamp":1711898507492,"user":{"displayName":"Somi","userId":"01643729748406509373"},"user_tz":-540},"id":"hP-WUX6Q54nU","outputId":"e798687c-e3c8-4eb6-9310-2de6fe2e7acd"},"outputs":[{"data":{"text/plain":["array([[<Axes: title={'center': 'similar'}>]], dtype=object)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["result.hist()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
